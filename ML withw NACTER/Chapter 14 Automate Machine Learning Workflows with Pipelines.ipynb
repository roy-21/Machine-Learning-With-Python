{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb417977-3bc7-4e97-93ba-422b858ffa13",
   "metadata": {},
   "source": [
    "# Chapter 14--> Automate Machine Learning Workflows with Pipelines\n",
    "\n",
    "There are standard workflows in a machine learning project that can be automated. In Python\n",
    "scikit-learn, Pipelines help to clearly define and automate these workflows. In this chapter you\n",
    "will discover Pipelines in scikit-learn and how you can automate common machine learning\n",
    "workflows. After completing this lesson you will know:\n",
    "1. How to use pipelines to minimize data leakage.\n",
    "2. How to construct a data preparation and modeling pipeline.\n",
    "3. How to construct a feature extraction and modeling pipeline.\n",
    "\n",
    "### The pipeline is defined with two steps:\n",
    "\n",
    "1. Standardize the data.\n",
    "2. Learn any ML model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee29a08-eba0-4c13-b6f3-41983149a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.3462064251538\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that standardizes the data then creates a model\n",
    "# Data Preparation and Modeling Pipeline (standardization + modeling)\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# load data\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "#print(estimators)\n",
    "\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148bdd4-83dd-470b-bd83-9539f3ae26a8",
   "metadata": {},
   "source": [
    "# Feature Extraction and Modeling Pipeline\n",
    "\n",
    "## feature extraction + modeling\n",
    "\n",
    "Feature extraction is another procedure that is susceptible to data leakage. Like data preparation,\n",
    "feature extraction procedures must be restricted to the data in your training dataset. The\n",
    "pipeline provides a handy tool called the FeatureUnion which allows the results of multiple\n",
    "feature selection and extraction procedures to be combined into a larger dataset on which a\n",
    "model can be trained. Importantly, all the feature extraction and the feature union occurs\n",
    "within each fold of the cross validation procedure. The example below demonstrates the pipeline\n",
    "defined with four steps:\n",
    "\n",
    "1. Feature Extraction with Principal Component Analysis (3 features).\n",
    "2. Feature Extraction with Statistical Selection (6 features).\n",
    "3. Feature Union.\n",
    "4. Learn a Logistic Regression Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "231b6ffc-b7a6-49be-8a10-4814c7bcd9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.60423786739577\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# load data\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression(max_iter=1000)))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean()*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
